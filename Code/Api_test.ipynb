{
 "cells": [
  {
   "source": [
    "# This notebook will test how we score the model. Several different preprocessing steps need to be automated."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request\n",
    "import json\n",
    "import joblib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import database as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def policy_pull(policy_num, user, password):\n",
    "\n",
    "    # Using an f string to input the user and password\n",
    "    connstring = f'mysql+mysqlconnector://{user}:{password}@127.0.0.1:3306/claims'\n",
    "    # Engine is a factory for connection. The connection does not happen here\n",
    "    engine = create_engine(connstring, echo=False)\n",
    "    # Connection happens here. Be sure to close\n",
    "    dbConnection    = engine.connect()\n",
    "    # Reading the table into a dataframe\n",
    "    policy = pd.read_sql(\"select * from claims.test_dataset where policy_number = {}\".format(policy_num), dbConnection);\n",
    "\n",
    "    # Closing the connection\n",
    "    dbConnection.close()\n",
    "\n",
    "    return policy\n",
    "\n",
    "def cutoff_pull(user, password):\n",
    "    \n",
    "    # Using an f string to input the user and password\n",
    "    connstring = f'mysql+mysqlconnector://{user}:{password}@127.0.0.1:3306/claims'\n",
    "    # Engine is a factory for connection. The connection does not happen here\n",
    "    engine = create_engine(connstring, echo=False)\n",
    "    # Connection happens here. Be sure to close\n",
    "    dbConnection    = engine.connect()\n",
    "    # Reading the table into a dataframe\n",
    "    capping = pd.read_sql(\"select * from claims.cutoff_values\", dbConnection);\n",
    "    # Closing the connection\n",
    "    dbConnection.close()\n",
    "\n",
    "    return capping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we will load the model into memory\n",
    "\n",
    "path = r\"C:\\Users\\sands\\OneDrive\\Desktop\\II_MSDS_Data_Practicum\\Data\"\n",
    "grid = joblib.load(path + r'\\\\model_final_FINAL_Gridsearch.mdl')\n",
    "\n",
    "# Next, we will load the cut off values from the database. Our user only has select privilege on the claims database\n",
    "capping = db.cutoff_pull('fraudapi', 'password')\n",
    "\n",
    "# Loading the column names from training set...\n",
    "with open(path + '\\columns2.pkl', 'rb') as file:\n",
    "    columns = pickle.load(file)\n",
    "\n",
    "# Bringing our original columns back\n",
    "with open(path + '\\original_columns.pkl', 'rb') as file:\n",
    "    originalColumns = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to json \n",
    "# data = request.get_json()\n",
    "user = 'pthielma'\n",
    "password = 'pass'\n",
    "policy_num = '119513'\n",
    "\n",
    "# Finding the policy in database \n",
    "claim = db.policy_pull(policy_num, user, password)\n",
    "\n",
    "# Applying capping values... \n",
    "\n",
    "# Removing columns not in training\n",
    "claim = claim[originalColumns]\n",
    "\n",
    "# Grabbing only the numeric columns\n",
    "numericCols = pd.DataFrame(claim.select_dtypes(exclude=['object'])).columns\n",
    "\n",
    "for var in numericCols:  \n",
    "\n",
    "    feature = claim[var]\n",
    "\n",
    "    # Calculate boundaries\n",
    "    lower = capping.loc[capping.feature == var].lower.values[0]\n",
    "    upper = capping.loc[capping.feature == var].upper.values[0]\n",
    "    # Replace outliers\n",
    "    claim[var] = np.where(feature > upper, upper, np.where(feature < lower, lower, feature))\n",
    "\n",
    "# Loading in encoder...\n",
    "from sklearn import preprocessing\n",
    "import category_encoders as ce\n",
    "# le = preprocessing.LabelEncoder()\n",
    "\n",
    "# loading the encoder\n",
    "with open(path + '\\encoder.pkl', 'rb') as file:\n",
    "    le = pickle.load(file)\n",
    "\n",
    "# Here, we are dropping the target. Obviously, we wouldn't have a target in production :)\n",
    "claim.drop(['fraud_reported'], axis=1, inplace=True)\n",
    "\n",
    "claim = le.transform(claim)\n",
    "\n",
    "# Creating features...\n",
    "import featuretools as ft\n",
    "\n",
    "# Make an entityset and add the entity\n",
    "es = ft.EntitySet(id = 'claims')\n",
    "es.entity_from_dataframe(entity_id = 'data', dataframe = claim, \n",
    "                        make_index = True, index = 'index')\n",
    "\n",
    "# Run deep feature synthesis with transformation primitives\n",
    "feature_matrix, feature_defs = ft.dfs(entityset = es, target_entity = 'data',\n",
    "                                    trans_primitives = ['add_numeric', 'multiply_numeric'])\n",
    "\n",
    "# Predicting...\n",
    "prediction = grid.predict(feature_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-school]",
   "language": "python",
   "name": "conda-env-.conda-school-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}